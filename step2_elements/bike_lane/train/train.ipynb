{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54048b8-9742-4dac-9b63-48a687dabf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 4 GPUs. Auto-selecting GPU 0 with the most free memory.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Block 0: GPU and Environment Setup\n",
    "# =============================================================================\n",
    "# This block automatically selects the GPU with the most free memory to ensure\n",
    "# efficient training without manual configuration. It also sets up helper\n",
    "# functions for creating a deterministic (reproducible) training environment.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_freest_gpu():\n",
    "    \"\"\"\n",
    "    Finds the GPU with the most available memory using the `nvidia-smi` command.\n",
    "    This is useful for multi-GPU systems to avoid overloading a specific device.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query nvidia-smi for free memory per GPU, returned in MiB\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        # Parse the output and find the index of the GPU with max free memory\n",
    "        free_memories = [int(x) for x in result.strip().split('\\n')]\n",
    "        best_gpu_id = free_memories.index(max(free_memories))\n",
    "        print(f\"✅ Found {len(free_memories)} GPUs. Auto-selecting GPU {best_gpu_id} with the most free memory.\")\n",
    "        return best_gpu_id\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not run `nvidia-smi`. Defaulting to GPU 0. Error: {e}\")\n",
    "        return 0  # Fallback to GPU 0 if nvidia-smi fails\n",
    "\n",
    "# Set the visible CUDA device for this script\n",
    "try:\n",
    "    gpu_id = get_freest_gpu()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "except Exception:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # Fallback for non-GPU environments\n",
    "    print(\"⚠️ GPU not found. Defaulting to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8df9fd-f597-4936-a848-0f47de5ab4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 1: Determinism Utilities for Reproducibility\n",
    "# =============================================================================\n",
    "# These functions are critical for ensuring that model training is reproducible.\n",
    "# By setting the same seed, get the exact same results every time\n",
    "# =============================================================================\n",
    "\n",
    "def set_global_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Sets a global seed for Python, NumPy, and PyTorch to ensure that all\n",
    "    random operations are deterministic.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Configure cuDNN for deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_loader_kwargs(seed: int, num_workers: int = 4):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of arguments for a PyTorch DataLoader to make its\n",
    "    data shuffling and worker processes deterministic.\n",
    "    \"\"\"\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    def _seed_worker(worker_id):\n",
    "        # Each worker gets a unique seed\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return dict(\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=_seed_worker,\n",
    "        generator=g,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b8d10a-0d43-4ec3-8dde-e566584680e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a74e701e9a4dfe82560e5edc4e554f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Block 2: Configuration and Dataset Definition\n",
    "# =============================================================================\n",
    "# This block defines all configurable parameters, sets up the image processor,\n",
    "# and defines the custom PyTorch Dataset for loading our multimodal data.\n",
    "# =============================================================================\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# --- Main Configuration ---\n",
    "# Modify these paths and settings for your environment\n",
    "BASE_DIR = \"/home/slieu3/CS_bikelane/for_GIT_test/train\" # Base project directory\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, f\"data/image_set\")\n",
    "\n",
    "# Paths to the CSV files defining training and validation splits\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, f\"data/TRAIN.csv\")\n",
    "VAL_CSV_PATH = os.path.join(BASE_DIR, f\"data/VAL.csv\")\n",
    "\n",
    "# --- Dataset and Model Setup ---\n",
    "train_df_balanced = pd.read_csv(TRAIN_CSV_PATH)\n",
    "val_df = pd.read_csv(VAL_CSV_PATH)\n",
    "\n",
    "# Load the image processor from Hugging Face for the Swin Transformer model.\n",
    "# This handles resizing, normalization, and other preprocessing steps.\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-large-patch4-window12-384\")\n",
    "\n",
    "# Map class names to integer labels for the model\n",
    "label_map = {\"no_bike_lane\": 0, \"designated\": 1, \"protected\": 2}\n",
    "\n",
    "class MultiImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch Dataset to handle our specific multimodal input.\n",
    "    For each item, it loads and processes three images (two Street View, one Satellite)\n",
    "    and their corresponding label.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, image_processor):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_and_process_image(self, image_filename: str):\n",
    "        \"\"\"Helper function to open an image, convert to RGB, and process it.\"\"\"\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # The processor returns a tensor of shape [1, C, H, W], so we squeeze it\n",
    "        return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Fetches the sample at the given index.\n",
    "        Returns a tuple of: (gsv1_tensor, gsv2_tensor, sat_tensor, label_tensor)\n",
    "        \"\"\"\n",
    "        row = self.df.loc[idx]\n",
    "        \n",
    "        # Load and process each of the three images\n",
    "        gsv1_tensor = self._load_and_process_image(row[\"GSV1\"])\n",
    "        gsv2_tensor = self._load_and_process_image(row[\"GSV2\"])\n",
    "        sat_tensor = self._load_and_process_image(row[\"SAT\"])\n",
    "        \n",
    "        # The \"is_bike\" column already contains our integer labels (0, 1, 2)\n",
    "        label = int(row[\"is_bike\"])\n",
    "        \n",
    "        return gsv1_tensor, gsv2_tensor, sat_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4758728d-ea0d-4ef9-b245-e93a13a3b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 3: Model Architecture Definition\n",
    "# =============================================================================\n",
    "# This block defines the neural network architectures for the winning model:\n",
    "# 1. SwinSingleViewHier: A Swin Transformer for a single image modality with\n",
    "#    two heads for hierarchical classification.\n",
    "# 2. WeightedDecisionFusionHier: A module that performs a learnable, weighted\n",
    "#    average of the predictions from the three single-view models.\n",
    "# =============================================================================\n",
    "import torch.nn as nn\n",
    "from transformers import SwinModel\n",
    "\n",
    "\n",
    "class SwinSingleViewHier(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-modality model using a Swin Transformer backbone.\n",
    "    It has two separate classification heads to perform hierarchical classification:\n",
    "    - head_presence: Predicts presence vs. absence of a bike lane.\n",
    "    - head_type: Predicts designated vs. protected, for lanes that are present.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"microsoft/swin-large-patch4-window12-384\"):\n",
    "        super().__init__()\n",
    "        self.backbone = SwinModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze all backbone layers except for the last two for fine-tuning\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if \"layers.2\" in name or \"layers.3\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        # Head 1: Is a bike lane present? (No Lane vs. Lane)\n",
    "        self.head_presence = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, 2)\n",
    "        )\n",
    "        # Head 2: What type of bike lane is it? (Designated vs. Protected)\n",
    "        self.head_type = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512), nn.ReLU(), nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"Performs a forward pass and returns logits from both heads.\"\"\"\n",
    "        # Get feature embedding from the backbone\n",
    "        features = self.backbone(pixel_values).last_hidden_state.mean(dim=1)\n",
    "        # Pass features through both heads\n",
    "        logits_presence = self.head_presence(features)\n",
    "        logits_type = self.head_type(features)\n",
    "        return logits_presence, logits_type\n",
    "\n",
    "class WeightedDecisionFusionHier(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements decision-level fusion with learnable weights.\n",
    "    It takes the logits from the three single-view models (GSV1, GSV2, SAT)\n",
    "    and combines them using a weighted average. The weights are learned\n",
    "    during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize raw weights. Softmax will be applied to ensure they sum to 1.\n",
    "        self.raw_weights = nn.Parameter(torch.tensor([0.33, 0.33, 0.34], dtype=torch.float32))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, p1, p2, p3, t1, t2, t3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p1, p2, p3: Presence logits from GSV1, GSV2, and SAT models.\n",
    "            t1, t2, t3: Type logits from GSV1, GSV2, and SAT models.\n",
    "        \"\"\"\n",
    "        weights = self.softmax(self.raw_weights)\n",
    "        \n",
    "        # Apply weighted average to the presence logits\n",
    "        fused_presence = self.dropout(weights[0]*p1 + weights[1]*p2 + weights[2]*p3)\n",
    "        # Apply weighted average to the type logits\n",
    "        fused_type = self.dropout(weights[0]*t1 + weights[1]*t2 + weights[2]*t3)\n",
    "        \n",
    "        return fused_presence, fused_type\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_weights(self):\n",
    "        \"\"\"Returns the learned weights after softmax normalization.\"\"\"\n",
    "        return self.softmax(self.raw_weights).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a0a8f9-12ef-4a65-a460-7a8d8b0b49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 4: Loss, Prediction, and Training Helper Functions\n",
    "# =============================================================================\n",
    "# This block contains the core logic for training and evaluation:\n",
    "# - hierarchical_loss: A custom loss function for the two-stage task.\n",
    "# - hierarchical_predict: A function to derive final labels from the two heads.\n",
    "# - train_epoch / eval_epoch: Standard loops for one epoch of training/validation.\n",
    "# - save_combo_best: A utility to save the best model checkpoint.\n",
    "# =============================================================================\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import optim\n",
    "\n",
    "def hierarchical_loss(out_presence, out_type, y_true):\n",
    "    \"\"\"\n",
    "    Calculates the total loss for the hierarchical model.\n",
    "    It's the sum of two cross-entropy losses:\n",
    "    1. Loss for presence detection on ALL samples.\n",
    "    2. Loss for type classification on ONLY the samples where a bike lane is present.\n",
    "    \"\"\"\n",
    "    # y_true is {0: no-lane, 1: designated, 2: protected}\n",
    "    # Create binary labels for presence (0 if no-lane, 1 otherwise)\n",
    "    y_presence = (y_true > 0).long()\n",
    "    loss1 = nn.CrossEntropyLoss()(out_presence, y_presence)\n",
    "    \n",
    "    # Create a mask to select only samples with bike lanes\n",
    "    lane_mask = (y_true > 0)\n",
    "    if lane_mask.any():\n",
    "        # y_type labels: 0 for designated (label 1), 1 for protected (label 2)\n",
    "        y_type = (y_true[lane_mask] - 1).long() \n",
    "        loss2 = nn.CrossEntropyLoss()(out_type[lane_mask], y_type)\n",
    "    else:\n",
    "        # If no bike lanes in the batch, type loss is 0\n",
    "        loss2 = torch.tensor(0.0, device=out_presence.device)\n",
    "        \n",
    "    return loss1 + loss2\n",
    "\n",
    "@torch.no_grad()\n",
    "def hierarchical_predict(out_presence, out_type):\n",
    "    \"\"\"\n",
    "    Generates final predictions (0, 1, or 2) from the two sets of logits.\n",
    "    \"\"\"\n",
    "    # Step 1: Predict presence (0=no-lane, 1=lane)\n",
    "    pred_presence = out_presence.argmax(dim=1)\n",
    "    \n",
    "    # Step 2: Predict type (0=designated, 1=protected)\n",
    "    pred_type = out_type.argmax(dim=1)\n",
    "    \n",
    "    # Initialize final predictions with presence predictions\n",
    "    final_preds = pred_presence.clone()\n",
    "    \n",
    "    # For samples predicted as having a lane, update with the type prediction\n",
    "    lane_mask = (pred_presence == 1)\n",
    "    final_preds[lane_mask] = pred_type[lane_mask] + 1 # Map {0,1} -> {1,2}\n",
    "    \n",
    "    return final_preds\n",
    "\n",
    "# --- Train/Eval Epoch Functions ---\n",
    "def train_epoch(models, fusion_model, loader, optimizer, device):\n",
    "    \"\"\"Runs a single training epoch.\"\"\"\n",
    "    # Set all models to training mode\n",
    "    m1, m2, m3 = models\n",
    "    m1.train(); m2.train(); m3.train(); fusion_model.train()\n",
    "    \n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    \n",
    "    for gsv1_batch, gsv2_batch, sat_batch, y_batch in loader:\n",
    "        # Move data to the selected device\n",
    "        gsv1_batch, gsv2_batch, sat_batch, y_batch = gsv1_batch.to(device), gsv2_batch.to(device), sat_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through each single-view model\n",
    "        p1, t1 = m1(gsv1_batch)\n",
    "        p2, t2 = m2(gsv2_batch)\n",
    "        p3, t3 = m3(sat_batch)\n",
    "        \n",
    "        # Forward pass through the fusion layer\n",
    "        fused_presence, fused_type = fusion_model(p1, p2, p3, t1, t2, t3)\n",
    "        \n",
    "        # Calculate loss and perform backpropagation\n",
    "        loss = hierarchical_loss(fused_presence, fused_type, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = hierarchical_predict(fused_presence, fused_type)\n",
    "        total_correct += (preds == y_batch).sum().item()\n",
    "        total_samples += y_batch.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(models, fusion_model, loader, device):\n",
    "    \"\"\"Runs a single evaluation epoch.\"\"\"\n",
    "    # Set all models to evaluation mode\n",
    "    m1, m2, m3 = models\n",
    "    m1.eval(); m2.eval(); m3.eval(); fusion_model.eval()\n",
    "    \n",
    "    all_preds, all_true = [], []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for gsv1_batch, gsv2_batch, sat_batch, y_batch in loader:\n",
    "        gsv1_batch, gsv2_batch, sat_batch, y_batch = gsv1_batch.to(device), gsv2_batch.to(device), sat_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        p1, t1 = m1(gsv1_batch)\n",
    "        p2, t2 = m2(gsv2_batch)\n",
    "        p3, t3 = m3(sat_batch)\n",
    "        fused_presence, fused_type = fusion_model(p1, p2, p3, t1, t2, t3)\n",
    "        \n",
    "        # Calculate loss and make predictions\n",
    "        loss = hierarchical_loss(fused_presence, fused_type, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        preds = hierarchical_predict(fused_presence, fused_type)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return np.array(all_preds), np.array(all_true), avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149f9c6b-9f05-40bc-9c57-b2736c4f0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 5: Full Training Pipeline for a Single Seed\n",
    "# =============================================================================\n",
    "# This is the main function that orchestrates the entire training and\n",
    "# evaluation process for a single random seed. It includes data loading,\n",
    "# model building, the training loop, early stopping, and saving results.\n",
    "# =============================================================================\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "CLASS_NAMES = [\"No Bike Lane\", \"Designated\", \"Protected\"]\n",
    "\n",
    "def train_one_run(seed: int, config: dict):\n",
    "    \"\"\"\n",
    "    Executes a complete training and evaluation pipeline for a single seed.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The random seed for this run.\n",
    "        config (dict): A dictionary containing all hyperparameters and paths.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics and paths to saved artifacts.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*30} Starting Run for Seed: {seed} {'='*30}\")\n",
    "    \n",
    "    # 1. Set seed for reproducibility\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    # 2. Build datasets and dataloaders\n",
    "    train_dataset = MultiImageDataset(train_df_balanced, config[\"image_dir\"], image_processor)\n",
    "    val_dataset = MultiImageDataset(val_df, config[\"image_dir\"], image_processor)\n",
    "    \n",
    "    loader_kwargs = make_loader_kwargs(seed=seed, num_workers=config[\"num_workers\"])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, **loader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, **loader_kwargs)\n",
    "\n",
    "    # 3. Build models, optimizer, and scheduler\n",
    "    device = config[\"device\"]\n",
    "    m1 = SwinSingleViewHier().to(device)\n",
    "    m2 = SwinSingleViewHier().to(device)\n",
    "    m3 = SwinSingleViewHier().to(device)\n",
    "    models = (m1, m2, m3)\n",
    "    fusion_model = WeightedDecisionFusionHier().to(device)\n",
    "    \n",
    "    all_params = list(m1.parameters()) + list(m2.parameters()) + list(m3.parameters()) + list(fusion_model.parameters())\n",
    "    optimizer = optim.AdamW(all_params, lr=config[\"lr\"], eps=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # 4. Training loop with early stopping\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "        train_loss, train_acc = train_epoch(models, fusion_model, train_loader, optimizer, device)\n",
    "        y_pred, y_true, val_loss = eval_epoch(models, fusion_model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate metrics for this epoch\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "        val_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        weights = fusion_model.get_weights()\n",
    "        \n",
    "        print(f\"[Seed {seed} | Epoch {epoch:03d}] \"\n",
    "              f\"TrainLoss={train_loss:.4f} TrainAcc={train_acc:.4f} | \"\n",
    "              f\"ValLoss={val_loss:.6f} ValAcc={val_acc:.4f} ValF1={val_f1:.4f} | \"\n",
    "              f\"Weights GSV1={weights[0]:.3f} GSV2={weights[1]:.3f} SAT={weights[2]:.3f}\")\n",
    "        \n",
    "        # Save epoch results\n",
    "        history.append({\n",
    "            \"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss, \"val_acc\": val_acc, \"val_f1\": val_f1,\n",
    "            \"w_gsv1\": weights[0], \"w_gsv2\": weights[1], \"w_sat\": weights[2]\n",
    "        })\n",
    "\n",
    "        # Check for improvement and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save model checkpoint\n",
    "            # (Implementation for saving/loading state_dict omitted for brevity, but would go here)\n",
    "            print(f\"✅ New best model found with validation loss: {best_val_loss:.6f}. Saving checkpoint.\")\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⚠️ No improvement. Patience {patience_counter}/{config['patience']}\")\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "    \n",
    "    # 5. Save results and artifacts for this run\n",
    "    os.makedirs(config[\"save_dir\"], exist_ok=True)\n",
    "    \n",
    "    # Save per-epoch history\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(os.path.join(config[\"save_dir\"], f\"history_seed_{seed}.csv\"), index=False)\n",
    "    \n",
    "    # Final evaluation using the best model would occur here\n",
    "    # (Loading the saved best checkpoint and running eval_epoch one last time)\n",
    "    # For this script, we'll just use the results from the last best epoch.\n",
    "    final_metrics = classification_report(y_true, y_pred, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"val_acc\": final_metrics[\"accuracy\"],\n",
    "        \"macro_f1\": final_metrics[\"macro avg\"][\"f1-score\"],\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac48110-afed-4fd2-a828-28b5f310472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device: cuda\n",
      "\n",
      "============================== Starting Run for Seed: 2023 ==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e56a62d29e48a1be68b66e7f224f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b3d7258004e8d9c9355529f1709a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/791M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe466851bfa2410fa4f3c1fd6bc23efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/791M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slieu3/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed 2023 | Epoch 001] TrainLoss=0.6503 TrainAcc=0.7453 | ValLoss=0.550380 ValAcc=0.8611 ValF1=0.8299 | Weights GSV1=0.332 GSV2=0.332 SAT=0.335\n",
      "✅ New best model found with validation loss: 0.550380. Saving checkpoint.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m all_run_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     all_run_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# --- Aggregate and Summarize Results ---\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36mtrain_one_run\u001b[0;34m(seed, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 58\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     y_pred, y_true, val_loss \u001b[38;5;241m=\u001b[39m eval_epoch(models, fusion_model, val_loader, device)\n\u001b[1;32m     61\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[11], line 75\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(models, fusion_model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m p1, t1 \u001b[38;5;241m=\u001b[39m m1(gsv1_batch)\n\u001b[1;32m     74\u001b[0m p2, t2 \u001b[38;5;241m=\u001b[39m m2(gsv2_batch)\n\u001b[0;32m---> 75\u001b[0m p3, t3 \u001b[38;5;241m=\u001b[39m \u001b[43mm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43msat_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Forward pass through the fusion layer\u001b[39;00m\n\u001b[1;32m     78\u001b[0m fused_presence, fused_type \u001b[38;5;241m=\u001b[39m fusion_model(p1, p2, p3, t1, t2, t3)\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m, in \u001b[0;36mSwinSingleViewHier.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs a forward pass and returns logits from both heads.\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Get feature embedding from the backbone\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Pass features through both heads\u001b[39;00m\n\u001b[1;32m     48\u001b[0m logits_presence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_presence(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:1069\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1063\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[1;32m   1065\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1066\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m   1067\u001b[0m )\n\u001b[0;32m-> 1069\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:881\u001b[0m, in \u001b[0;36mSwinEncoder.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    872\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    873\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    874\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m         always_partition,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:801\u001b[0m, in \u001b[0;36mSwinStage.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    799\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    807\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:727\u001b[0m, in \u001b[0;36mSwinLayer.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    725\u001b[0m hidden_states_windows \u001b[38;5;241m=\u001b[39m window_partition(shifted_hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m    726\u001b[0m hidden_states_windows \u001b[38;5;241m=\u001b[39m hidden_states_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n\u001b[0;32m--> 727\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attn_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    732\u001b[0m     hidden_states_windows, attn_mask, head_mask, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    735\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bike_lane_earlyfusion/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:682\u001b[0m, in \u001b[0;36mSwinLayer.get_attn_mask\u001b[0;34m(self, height, width, dtype, device)\u001b[0m\n\u001b[1;32m    680\u001b[0m     mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m    681\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 682\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Block 6: Main Execution Block\n",
    "# =============================================================================\n",
    "# This is the entry point of the script. It sets up the experiment\n",
    "# configuration, iterates through multiple random seeds, and aggregates\n",
    "# the final results to report mean and standard deviation, ensuring a robust\n",
    "# [cite_start]evaluation of the model's performance. [cite: 102, 174]\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Experiment Configuration ---\n",
    "    training_config = {\n",
    "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        \"seeds\": [2023, 2024, 2025, 2026, 2027], # Run with multiple seeds for robustness\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 5e-5,\n",
    "        \"num_epochs\": 80,\n",
    "        \"patience\": 15, # For early stopping\n",
    "        \"num_workers\": 4,\n",
    "        \"image_dir\": IMAGE_DIR,\n",
    "        \"save_dir\": OUTPUT_DIR,\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting training on device: {training_config['device']}\")\n",
    "\n",
    "    # --- Run Training for Each Seed ---\n",
    "    all_run_results = []\n",
    "    for seed in training_config[\"seeds\"]:\n",
    "        result = train_one_run(seed=seed, config=training_config)\n",
    "        all_run_results.append(result)\n",
    "\n",
    "    # --- Aggregate and Summarize Results ---\n",
    "    results_df = pd.DataFrame(all_run_results)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*30 + \" FINAL RESULTS \" + \"=\"*30)\n",
    "    print(\"\\nPer-seed performance:\\n\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Calculate mean and standard deviation of key metrics\n",
    "    summary = results_df.agg({\n",
    "        \"val_acc\": [\"mean\", \"std\"],\n",
    "        \"macro_f1\": [\"mean\", \"std\"]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSummary (mean ± std):\\n\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Save the aggregated and summary results to CSV files\n",
    "    results_df.to_csv(os.path.join(training_config[\"save_dir\"], \"multiseed_results.csv\"), index=False)\n",
    "    summary.to_csv(os.path.join(training_config[\"save_dir\"], \"multiseed_summary.csv\"))\n",
    "    \n",
    "    print(f\"\\n✅ Training complete. All results saved to: {training_config['save_dir']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bike_lane_earlyfusion)",
   "language": "python",
   "name": "bike_lane_earlyfusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
