{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd439da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup ###\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import requests\n",
    "import yaml\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import psutil\n",
    "import mapclassify\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpltPath\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from ultralytics import YOLO, settings\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "from branca.element import Template, MacroElement\n",
    "\n",
    "\n",
    "### Directories ###\n",
    "WORK_DIR = \".\"\n",
    "#API_KEY=\"\" # for GSV API\n",
    "CHECKPOINT_PATH = os.path.join(\"./model_sign_detection.pt\") # load the trained sign detection model\n",
    "\n",
    "\n",
    "### Import ###\n",
    "input_dir = \"./../../outputs/step1_preprocessing\"\n",
    "lines = gpd.read_file(os.path.join(input_dir, \"LINE_EPSG4326.geojson\"), driver=\"GeoJSON\")\n",
    "print(len(lines))\n",
    "#lines.iloc[0:5]\n",
    "\n",
    "points = gpd.read_file(os.path.join(input_dir, \"POINT_EPSG4326.geojson\"), driver=\"GeoJSON\")\n",
    "print(len(points))\n",
    "points.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694fa2c9-2cf6-4cfb-9638-1a81e9f2dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust heading based on road bearing vs pano_heading ###\n",
    "def get_side_heading(row, YOUR_ANGLE):\n",
    "    bearing = row['bearing']\n",
    "    pano_heading = row['pano_heading']\n",
    "\n",
    "    diff = abs(pano_heading - bearing)\n",
    "    diff = diff if diff <= 180 else 360 - diff  # shortest angular distance\n",
    "\n",
    "    if diff <= 45 or diff >= 315:\n",
    "        pano_heading_adjusted = pano_heading\n",
    "    else:\n",
    "        pano_heading_adjusted = pano_heading + 180\n",
    "    pano_heading_adjusted = pano_heading_adjusted % 360   # keep in [0, 360)\n",
    "    \n",
    "    if str(row[\"median\"]) == \"yes\":\n",
    "        if str(row[\"side\"]) == \"side1\":\n",
    "            pano_heading_side = pano_heading_adjusted + YOUR_ANGLE   \n",
    "        else:\n",
    "            pano_heading_side = pano_heading_adjusted + YOUR_ANGLE    \n",
    "    else:  # no median, one single centerline for both sides\n",
    "        if str(row[\"side\"]) == \"side1\":\n",
    "            pano_heading_side = pano_heading_adjusted + YOUR_ANGLE   \n",
    "        else:\n",
    "            pano_heading_side = pano_heading_adjusted + YOUR_ANGLE # I already switched the bearing to the other side before \n",
    "    return pd.Series({\"pano_heading_side\": pano_heading_side})\n",
    "\n",
    "side_headings = points.apply(lambda row: get_side_heading(row, 40), axis=1)\n",
    "points = pd.concat([points, side_headings], axis=1)\n",
    "\n",
    "points = points.sort_values(by=[\"link_id\", \"side\", \"point_id\"])\n",
    "print(len(points))\n",
    "print(sum(points[\"pano_heading\"].astype(float).isna()))\n",
    "print(sum(points[\"pano_heading_side\"].astype(float).isna()))\n",
    "points.iloc[0:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d2b613-d8fc-4ca7-9ba2-55396f6d5f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'point_whole_side_assigned_azi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define ranges\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m MAX_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mpoint_whole_side_assigned_azi\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_ranges\u001b[39m(start, stop, interval, num_ranges, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH):\n\u001b[1;32m      4\u001b[0m     ranges \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'point_whole_side_assigned_azi' is not defined"
     ]
    }
   ],
   "source": [
    "### GSV download ###\n",
    "MAX_LENGTH = len(points)\n",
    "def generate_ranges(start, stop, interval, num_ranges, max_length=MAX_LENGTH):\n",
    "    ranges = []\n",
    "    for i in range(num_ranges):\n",
    "        range_start = start + i * interval\n",
    "        range_end = start + (i + 1) * interval\n",
    "        range_end = min(range_end, max_length)    \n",
    "        ranges.append(range(range_start, range_end))\n",
    "    return ranges\n",
    "ranges = generate_ranges(0, 1000, 1000, 18, MAX_LENGTH)\n",
    "print(ranges[0]) \n",
    "print(ranges[1]) \n",
    "print(\"...\")\n",
    "print(ranges[-2]) \n",
    "print(ranges[-1]) \n",
    "print()\n",
    "\n",
    "\n",
    "output_dir = os.path.join(\".\", \"outputs\") \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "\n",
    "def get_image(row):\n",
    "    linkid = row['link_id']\n",
    "    pointid = row['point_id']\n",
    "    location = f\"{round(row['pano_lat'], 9)},{round(row['pano_lon'], 9)}\"\n",
    "    side = row['side']\n",
    "    heading = round(row['pano_heading_side'], 3)\n",
    "    date = row['pano_date']\n",
    "    panoid = row['pano_id']\n",
    "    width = 640\n",
    "    height = 640\n",
    "    fov = 100\n",
    "    endpoint = \"https://streetviewpixels-pa.googleapis.com/v1/thumbnail?cb_client=maps_sv.tactile&\"\n",
    "    furl = f\"{endpoint}w={width}&h={height}&pitch={0}&panoid={panoid}&yaw={heading}&thumbfov={fov}\"\n",
    "    fname = f\"gsv__{linkid}__{side}__{pointid}__{date}__{heading}__{location}.jpg\"  # Don't change this naming\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "    if not os.path.exists(output_path):\n",
    "        response = requests.get(furl, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Error: Failed to fetch {output_path}, Status Code: {response.status_code}\")\n",
    "for r in ranges:\n",
    "    print(r)\n",
    "    points_download = points.iloc[r]\n",
    "    print(points_download.shape)\n",
    "    for i in range(len(points_download)):\n",
    "        row = points_download.iloc[i]\n",
    "        get_image(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f55876-6210-4e28-941a-4d5cd5c0a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings for Sign Detection (GPU needed) ###\n",
    "import torch  \n",
    "print(torch.cuda.is_available())  \n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2:.2f} MB\")\n",
    "        print(f\"  Memory Cached:    {torch.cuda.memory_reserved(i)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No CUDA-compatible GPU detected.\")\n",
    "\n",
    "from ultralytics import YOLO settings\n",
    "#ultralytics.checks()\n",
    "\n",
    "def install_opencv_headless():\n",
    "    try:\n",
    "        subprocess.run([\"pip\", \"install\", \"opencv-python-headless\"], check=True)\n",
    "        print(\"opencv-python-headless installed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "#install_opencv_headless()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221ffcb0-d4ff-438b-8d24-eaf27db90ee5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load images\u001b[39;00m\n\u001b[1;32m      2\u001b[0m folder_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      4\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(filenames))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs'"
     ]
    }
   ],
   "source": [
    "### Prepare for Sign Detection ###\n",
    "# load images\n",
    "folder_dir = \"./outputs\"\n",
    "filenames = sorted([x for x in os.listdir(folder_dir) if x.endswith(\".jpg\")])\n",
    "filenames = [os.path.join(folder_dir, f) for f in filenames]\n",
    "print(\"Total files:\", len(filenames))\n",
    "\n",
    "\n",
    "# set your batch size (e.g., 1000 per range)\n",
    "BATCH_SIZE = 1000\n",
    "MAX_LENGTH = len(filenames)\n",
    "def generate_ranges(batch_size, max_length=MAX_LENGTH):\n",
    "    num_batches = ceil(max_length / batch_size)\n",
    "    ranges = []\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, max_length)\n",
    "        ranges.append(range(start, end))\n",
    "    return ranges\n",
    "ranges = generate_ranges(BATCH_SIZE)\n",
    "print(\"Number of ranges:\", len(ranges))\n",
    "\n",
    "\n",
    "# create a storage directory of GSV images\n",
    "output_dir = os.path.join(\".\", \"sign_detected_imgs\") \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(output_dir)\n",
    "\n",
    "\n",
    "# define classes\n",
    "classes =  {\n",
    "    \"class_id\": [0, 1, 2, 3, 4],\n",
    "    \"class_nm\": [\"prohibAnytime\", \n",
    "                 \"prohibSometimes\", \n",
    "                 \"prohibForBusStop\", \n",
    "                 \"prohibForCleaning\", \n",
    "                 \"permit\"]}\n",
    "classes = pd.DataFrame(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1123645-5f25-4b83-b19e-d04a5efbeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sign Detection ###\n",
    "# ---------- SETTINGS ----------\n",
    "THRES_CONFID = 0.3\n",
    "THRES_AREA = 200\n",
    "BATCH_SIZE = 8\n",
    "command = [\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv,nounits\"]\n",
    "\n",
    "# ---------- LOAD MODEL ON GPU 3 ----------\n",
    "torch.cuda.set_device(3)  \n",
    "model = YOLO(input_path).to(\"cuda:3\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"       # ðŸ‘ˆ make GPU 3 the only visible one\n",
    "# torch.cuda.set_device(0)    \n",
    "# model = YOLO(input_path).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "log_detected = []\n",
    "\n",
    "# ---------- PROCESS RANGES SEQUENTIALLY ----------\n",
    "for i, batch_range in enumerate(ranges):\n",
    "    print(f\"{i}th range starts: {batch_range}\")\n",
    "\n",
    "    files = filenames[batch_range[0]:batch_range[-1]]\n",
    "    try:\n",
    "        results = model.predict(\n",
    "            files,\n",
    "            conf=THRES_CONFID,\n",
    "            #device=\"cuda:0\",    # refers to GPU 3\n",
    "            save=False,\n",
    "            verbose=False,\n",
    "            batch=BATCH_SIZE,\n",
    "            half=True\n",
    "        )\n",
    "\n",
    "        batch_log = []\n",
    "        for k, result in enumerate(results):\n",
    "            boxes = result.boxes.data\n",
    "            fname = os.path.basename(files[k])\n",
    "\n",
    "            for j, box in enumerate(boxes):\n",
    "                xmin, ymin, xmax, ymax, confidence, class_id = box.tolist()\n",
    "                xmin, ymin, xmax, ymax, confidence = int(xmin), int(ymin), int(xmax), int(ymax), round(confidence, 2)\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "                class_id = int(class_id)\n",
    "                class_nm = classes.iloc[class_id][\"class_nm\"]\n",
    "\n",
    "                if confidence > THRES_CONFID and area > THRES_AREA:\n",
    "                    result.save(os.path.join(output_dir, fname))\n",
    "                    batch_log.append({\n",
    "                        \"file_id\": fname,\n",
    "                        \"detection_index\": j,\n",
    "                        \"class_id\": class_id,\n",
    "                        \"class_nm\": class_nm,\n",
    "                        \"xmin\": xmin,\n",
    "                        \"ymin\": ymin,\n",
    "                        \"xmax\": xmax,\n",
    "                        \"ymax\": ymax,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"area\": area\n",
    "                    })\n",
    "\n",
    "        log_detected.extend(batch_log)\n",
    "        print(f\"Batch {i}: {len(batch_log)} detections\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}: {e}\")\n",
    "\n",
    "    # free GPU memory between batches\n",
    "    del results, files, batch_log\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    shell_op = subprocess.run(command, stdout=subprocess.PIPE, text=True)\n",
    "    #print(f\"GPU memory after batch {i}:\\n{shell_op.stdout}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b49662-bdfe-4f2a-98bf-ceb1886e53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_detected_df = pd.DataFrame(log_detected)\n",
    "log_detected_df = log_detected_df[['file_id', 'class_nm', 'confidence', 'area',\n",
    "                                       'detection_index', 'class_id', \n",
    "                                       'xmin', 'ymin', 'xmax', 'ymax']]\n",
    "log_detected_df[\"link_id\"] = log_detected_df[\"file_id\"].apply(lambda x: x.split(\"__\")[1])\n",
    "log_detected_df[\"side\"] = log_detected_df[\"file_id\"].apply(lambda x: x.split(\"__\")[2])\n",
    "log_detected_df[\"point_id\"] = log_detected_df[\"file_id\"].apply(lambda x: int(x.split(\"__\")[3]))\n",
    "log_detected_df = log_detected_df[['link_id', 'side', 'point_id', \n",
    "                                   'class_nm', 'confidence', 'area',\n",
    "                                   'detection_index', 'class_id',\n",
    "                                   'xmin', 'ymin', 'xmax', 'ymax', 'file_id']]\n",
    "log_detected_df = log_detected_df.sort_values(by=['link_id', 'side', 'point_id'])\n",
    "print(len(log_detected_df))\n",
    "print(log_detected_df.groupby(\"class_nm\")[\"class_nm\"].count())\n",
    "log_detected_df.iloc[0:3]\n",
    "\n",
    "output_path = os.path.join(WORK_DIR, \"sign_inferred.csv\")\n",
    "log_detected_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7261ef-9fe3-4293-bb23-15690797a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_side1 = points[points[\"side\"] == \"side1\"][[\"link_id\", \"side\", \"point_id\", \"pano_id\", \"pano_date\", \"geometry\"]]\n",
    "print(len(points_side1))\n",
    "\n",
    "class_order = [    \n",
    "    \"prohibAnytime\",\n",
    "    \"prohibSometimes\",\n",
    "    \"prohibForBusStop\",\n",
    "    #\"prohibForCleaning\",\n",
    "    \"permit\",\n",
    "]\n",
    "\n",
    "points_side1_detected = points_side1.merge(log_detected_df[log_detected_df[\"side\"]==\"side1\"],\n",
    "                                 on=[\"link_id\", \"side\", \"point_id\"],\n",
    "                                 how=\"left\")\n",
    "\n",
    "points_side1_detected = points_side1_detected[~points_side1_detected[\"confidence\"].isna()]\n",
    "\n",
    "points_side1_detected[\"class_nm\"] = pd.Categorical(\n",
    "    points_side1_detected[\"class_nm\"],\n",
    "    categories=class_order,\n",
    "    ordered=True\n",
    ")\n",
    "points_side1_detected = points_side1_detected.sort_values(by=\"class_nm\")\n",
    "points_side1_detected[\"class_nm\"] = points_side1_detected[\"class_nm\"].astype(str)\n",
    "\n",
    "print(len(points_side1_detected))\n",
    "print(points_side1_detected.iloc[0:2])\n",
    "print()\n",
    "\n",
    "sign_types = points_side1_detected[\"class_nm\"].unique()\n",
    "print(sign_types)\n",
    "colors = plt.get_cmap(\"RdYlBu\")(np.linspace(0, 1, len(sign_types)))\n",
    "sign_types_color_map = dict(zip(sign_types, [mcolors.to_hex(c) for c in colors]))\n",
    "points_side1_detected[\"color\"] = points_side1_detected[\"class_nm\"].map(sign_types_color_map)\n",
    "\n",
    "m = atl.explore(\n",
    "    color=\"white\",\n",
    "    alpha=1,\n",
    "    name=\"atl_metro\",\n",
    "    tooltip=False,\n",
    "    control_scale=True\n",
    ")\n",
    "\n",
    "m = points_side1.explore(\n",
    "    m=m,\n",
    "    color=\"grey\",\n",
    "    name=\"Pano IDs\",\n",
    "    marker_kwds={\"radius\": 1.0, \"fillOpacity\": 0.4})\n",
    "\n",
    "m = points_side1_detected.explore(\n",
    "    m=m,\n",
    "    color=points_side1_detected[\"color\"],\n",
    "    name=\"Detected signs by category\",\n",
    "    tooltip=\"class_nm\",\n",
    "    marker_kwds={\"radius\": 2.0, \"fillOpacity\": 0.8}\n",
    ")\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "legend_entries = \"\\n\".join([\n",
    "    f'<span style=\"color:{color};\">&#9632;</span> {rtype}<br>'\n",
    "    for rtype, color in sign_types_color_map.items()\n",
    "])\n",
    "\n",
    "legend_html = f\"\"\"\n",
    "{{% macro html(this, kwargs) %}}\n",
    "<div style=\"\n",
    "    position: fixed; \n",
    "    top: 100px; left: 5px; width: 190px; height: 160px;\n",
    "    background-color: white;\n",
    "    border:2px solid grey;\n",
    "    z-index:9999;\n",
    "    font-size:14px;\n",
    "    padding: 10px;\n",
    "    box-shadow: 2px 2px 6px rgba(0,0,0,0.3);\n",
    "    line-height: 1.5em;\n",
    "    \">\n",
    "    <b>Sign Category</b><br>\n",
    "    {legend_entries}\n",
    "</div>\n",
    "{{% endmacro %}}\n",
    "\"\"\"\n",
    "\n",
    "legend = MacroElement()\n",
    "legend._template = Template(legend_html)\n",
    "m.get_root().add_child(legend)\n",
    "\n",
    "#m.save(\"./map.html\")\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (jlstpksign)",
   "language": "python",
   "name": "jlstpksign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
